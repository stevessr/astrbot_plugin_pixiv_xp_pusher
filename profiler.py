"""
XP ç”»åƒæ„å»ºæ¨¡å—
åˆ†ææ”¶è—Tagï¼Œæ„å»ºç”¨æˆ·XPæƒé‡
"""
import logging
import math
import itertools
import json
import asyncio
import re
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from typing import Optional

try:
    from openai import AsyncOpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

from pixiv_client import Illust, PixivClient
import database as db
from utils import retry_async



logger = logging.getLogger(__name__)


# å¸¸è§åˆ«åæ˜ å°„è¡¨ï¼ˆå¯æ‰©å±•ï¼‰
TAG_ALIASES = {
    "ç™½é«ª": "white_hair",
    "silver hair": "white_hair",
    "ç™½é«®": "white_hair",
    "çŒ«è€³": "cat_ears",
    "cat ears": "cat_ears",
    "nekomimi": "cat_ears",
    "ãƒ­ãƒª": "loli",
    "å·¨ä¹³": "large_breasts",
    "ãŠã£ã±ã„": "breasts",
    "é»’é«ª": "black_hair",
    "é‡‘é«ª": "blonde_hair",
    "ãƒ„ã‚¤ãƒ³ãƒ†ãƒ¼ãƒ«": "twintails",
    "twin tails": "twintails",
    "ãƒ¡ã‚¤ãƒ‰": "maid",
    "æ°´ç€": "swimsuit",
    "åˆ¶æœ": "uniform",
    "ã‚¹ãƒˆãƒƒã‚­ãƒ³ã‚°": "stockings",
    "ãƒ‹ãƒ¼ã‚½": "thighhighs",
    "çœ¼é¡": "glasses",
}


class AITagProcessor:
    """AI Tag å¤„ç†å™¨ - è¿‡æ»¤æ— æ„ä¹‰tagå’Œå½’ç±»åŒä¹‰tag"""
    
    def __init__(self, config: dict):
        self.enabled = config.get("enabled", False) and HAS_OPENAI
        self.filter_meaningless = config.get("filter_meaningless", True)
        self.merge_synonyms = config.get("merge_synonyms", True)
        self.model = config.get("model", "gpt-4o-mini")
        self.batch_size = config.get("batch_size", 50)
        self.concurrency = config.get("concurrency", 3)  # å¹¶å‘æ•°
        self.pattern_users = re.compile(r"^(.*?)\d+userså…¥ã‚Š$") # é¢„ç¼–è¯‘æ­£åˆ™
        
        if self.enabled:
            self.client = AsyncOpenAI(
                api_key=config.get("api_key", ""),
                base_url=config.get("base_url") or None
            )
        else:
            self.client = None
        
        # ç¼“å­˜å¤„ç†ç»“æœ (Tag -> CleanedTag/None)
        self._cache: dict[str, str | None] = {}
        self._cache_initialized = False
        # è®°å½•å‘ç”Ÿçš„é”™è¯¯
        self.occurred_errors: list[int] = []
    
    def _preprocess_tags(self, tags: list[str]) -> list[str]:
        """æ­£åˆ™é¢„å¤„ç†ï¼šå»é™¤ userså…¥ã‚Š åç¼€ç­‰"""
        processed = []
        for tag in tags:
            # 1. å»é™¤ userså…¥ã‚Š
            match = self.pattern_users.match(tag)
            if match:
                prefix = match.group(1)
                # å¦‚æœå‰ç¼€éç©ºï¼Œåˆ™ä½¿ç”¨å‰ç¼€ï¼ˆä¾‹å¦‚ "é¸£æ½®"ï¼‰ï¼›å¦åˆ™ä¿ç•™åŸæ ·
                processed.append(prefix if prefix else tag)
            else:
                processed.append(tag)
        return processed

    async def process_tags(self, tags: list[str]) -> tuple[list[str], dict[str, str]]:
        if not self.enabled or not tags:
            return tags, {}
            
        # 0. æ­£åˆ™é¢„å¤„ç†
        effective_tags = self._preprocess_tags(tags)
        
        # 1. é¦–æ¬¡è¿è¡Œæ—¶åŠ è½½ DB ç¼“å­˜
        if not self._cache_initialized:
            try:
                db_cache = await db.get_ai_cache_map()
                self._cache.update(db_cache)
                self._cache_initialized = True
                logger.info(f"å·²åŠ è½½ {len(db_cache)} æ¡ AI æ ‡ç­¾ç¼“å­˜")
            except Exception as e:
                logger.error(f"åŠ è½½ AI ç¼“å­˜å¤±è´¥: {e}")
        
        # 2. æ£€æŸ¥ç¼ºå¤± (ä½¿ç”¨é¢„å¤„ç†åçš„æ ‡ç­¾)
        uncached = [t for t in effective_tags if t not in self._cache]
        
        if uncached:
            # å»é‡
            uncached = list(set(uncached))
            logger.info(f"å‘ç° {len(uncached)} ä¸ªæ–° Tagï¼Œå¼€å§‹ AI å¤„ç† ({self.batch_size}/æ‰¹)...")
            await self._batch_process(uncached)
        
        # 3. åº”ç”¨ç»“æœ
        valid_tags = []
        synonym_map = {}
        
        for i, original_tag in enumerate(tags):
            effective_tag = effective_tags[i]
            
            # å®‰å…¨è·å–ç»“æœ
            result = self._cache.get(effective_tag, effective_tag)
            
            if result is None:
                continue # meaningless
            
            # åªè¦ç»“æœä¸åŸå§‹æ ‡ç­¾ä¸åŒï¼Œå°±è®°å½•æ˜ å°„
            if result != original_tag:
                synonym_map[original_tag] = result
            
            valid_tags.append(result)
        
        # å»é‡ä¿æŒé¡ºåº
        valid_tags = list(dict.fromkeys(valid_tags))
        
        return valid_tags, synonym_map

    @retry_async(max_retries=3, delay=2.0)
    async def _call_api(self, prompt: str) -> str:
        """è°ƒç”¨AI APIï¼ˆæµå¼ï¼Œé˜²æ­¢è¶…æ—¶ï¼‰"""
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªPixivæ’ç”»æ ‡ç­¾æ•°æ®å¤„ç†ä¸“å®¶ï¼Œåªè¾“å‡ºæ ‡å‡†JSONã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            response_format={"type": "json_object"},
            stream=True  # å¯ç”¨æµå¼
        )
        
        collected_content = []
        async for chunk in response:
            if chunk.choices and chunk.choices[0].delta.content:
                collected_content.append(chunk.choices[0].delta.content)
                
        return "".join(collected_content)

    async def _batch_process(self, tags: list[str]):
        """å¹¶å‘æ‰¹é‡å¤„ç†Tag"""
        if not tags:
            return
        
        batches = []
        for i in range(0, len(tags), self.batch_size):
            batches.append(tags[i:i + self.batch_size])
            
        logger.info(f"é˜Ÿåˆ—å…± {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œå¹¶å‘æ•°: {self.concurrency}")
        
        semaphore = asyncio.Semaphore(self.concurrency)
        
        async def _bounded_Process(batch):
            async with semaphore:
                await self._process_single_batch(batch)
                
        tasks = [_bounded_Process(b) for b in batches]
        await asyncio.gather(*tasks)

    async def _process_single_batch(self, tags: list[str]):
        """å¤„ç†å•æ‰¹æ¬¡å¹¶æŒä¹…åŒ–"""
        prompt = self._build_prompt(tags)
        
        last_error = None
        max_logic_retries = 5
        
        for attempt in range(max_logic_retries):
            try:
                logger.info(f"ğŸ¤– æ­£åœ¨è¯·æ±‚ AIæ¸…æ´— (å°è¯• {attempt+1}/{max_logic_retries})...")
                content = await self._call_api(prompt)
                
                # æ¸…æ´— Markdown
                content = content.strip()
                if content.startswith("```json"): content = content[7:]
                if content.startswith("```"): content = content[3:]
                if content.endswith("```"): content = content[:-3]
                
                # ç›´æ¥è§£æï¼Œå¤±è´¥åˆ™è§¦å‘å¤–å±‚é‡è¯•
                result = json.loads(content)
    
                meaningless = set(result.get("meaningless", []))
                synonyms = result.get("synonyms", {})
                
                # æ„é€ æ›´æ–°æ˜ å°„
                cache_update = {}
                mapping_update = {} 
                
                for tag in tags:
                    if tag in meaningless:
                        cache_update[tag] = None
                    elif tag in synonyms:
                        cleaned = synonyms[tag]
                        cache_update[tag] = cleaned
                        mapping_update[tag] = cleaned
                    else:
                        cache_update[tag] = tag
                
                # æ›´æ–°å†…å­˜ç¼“å­˜
                self._cache.update(cache_update)
                
                # æŒä¹…åŒ–åˆ° DB
                await db.update_ai_cache(cache_update) 
                if mapping_update:
                    await db.update_tag_mapping_stats(mapping_update)
                    
                # ç¾åŒ–æ—¥å¿—è¾“å‡º
                logger.info(f"âœ¨ AI Batch å®Œæˆ (æœ¬æ‰¹ {len(tags)} ä¸ª)")
                
                if meaningless:
                    logger.info(f"   ğŸ—‘ï¸ è¿‡æ»¤ {len(meaningless)} ä¸ªæ ‡ç­¾")
                if synonyms:
                    logger.info(f"   ğŸ”„ å½’ç±» {len(synonyms)} ä¸ªæ ‡ç­¾")
                
                # æˆåŠŸåˆ™ç›´æ¥è¿”å›
                return

            except Exception as e:
                last_error = e
                # ç®€åŒ–æŠ¥é”™æ—¥å¿—
                error_msg = str(e)
                if "524" in error_msg:
                    logger.warning(f"AI APIè¶…æ—¶ (524)")
                else:
                    logger.warning(f"AIå¤„ç†æ‰¹æ¬¡å¤±è´¥ (å°è¯• {attempt+1}): {e}")
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
                if attempt < max_logic_retries - 1:
                    await asyncio.sleep(2)
                    continue

        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œæ‰§è¡Œæœ€ç»ˆçš„é”™è¯¯å¤„ç†
        if last_error:
            logger.error(f"âŒ AI Batch æœ€ç»ˆå¤±è´¥: {last_error}")
            
            # è®°å½•é”™è¯¯åˆ°æ•°æ®åº“ (ä»…éè¶…æ—¶é”™è¯¯)
            if "524" not in str(last_error):
                try:
                    err_id = await db.add_ai_error(tags, str(last_error))
                    self.occurred_errors.append(err_id)
                except Exception as db_e:
                    logger.error(f"è®°å½•é”™è¯¯æ—¥å¿—å¤±è´¥: {db_e}")
            
            # å¤±è´¥æ—¶ä¿ç•™æ‰€æœ‰tags (Fallback)
            for tag in tags:
                self._cache[tag] = tag
    
from utils import TAG_TRANSLATIONS


def _build_ai_prompt(tags: list[str]) -> str:
    """æ„å»ºä¼˜åŒ–åçš„ AI æç¤ºè¯"""
    
    # æ„å»ºå®Œæ•´çš„æ ‡å‡†è¯åº“ï¼ˆä¾› AI å‚è€ƒï¼‰
    canonical_dict = {}
    for canonical, query_str in TAG_TRANSLATIONS.items():
        aliases = [p.strip().replace("(", "").replace(")", "") for p in query_str.split(" OR ")]
        for alias in aliases:
            if alias != canonical:
                canonical_dict[alias] = canonical
    
    # æ ¼å¼åŒ–è¯åº“ä¸ºç´§å‡‘å­—ç¬¦ä¸²
    dict_entries = [f"{k} â†’ {v}" for k, v in canonical_dict.items()]
    dict_text = " | ".join(dict_entries)

    return f"""# Pixiv Tag æ¸…æ´—ä»»åŠ¡

## ç›®æ ‡
å°†åŸå§‹ Pixiv æ ‡ç­¾æ¸…æ´—ä¸ºé€‚åˆç”¨æˆ·ç”»åƒåˆ†æçš„æ ‡å‡†åŒ–æ ‡ç­¾ã€‚

## å·²æœ‰æ ‡å‡†è¯åº“ï¼ˆå¿…é¡»ä¼˜å…ˆä½¿ç”¨ï¼‰
ä»¥ä¸‹æ˜¯ç³»ç»Ÿå·²å®šä¹‰çš„æ ‡å‡†æ˜ å°„ï¼Œå¦‚æœè¾“å…¥åŒ…å«è¿™äº›æ ‡ç­¾ï¼Œ**å¿…é¡»**ä½¿ç”¨å¯¹åº”çš„æ ‡å‡†å½¢å¼ï¼š
{dict_text}

## è§„åˆ™

### 1. è¿‡æ»¤ï¼ˆæ”¾å…¥ meaninglessï¼‰
åˆ é™¤ä»¥ä¸‹ç±»å‹çš„æ ‡ç­¾ï¼š
- **å¹³å°/ç»Ÿè®¡ç±»**: original, pixiv, userså…¥ã‚Š, bookmark, åè—, ä»•äº‹çµµ
- **åˆ›ä½œè¿‡ç¨‹ç±»**: è½æ›¸ã, ç·´ç¿’, WIP, sketch, doodle
- **å®£ä¼ /è¯·æ±‚ç±»**: ãŠä»•äº‹å‹Ÿé›†ä¸­, commission, request, follow me
- **å¹´ä»½/æ—¥æœŸç±»**: 2024, 2023, æ–°å¹´, ã‚¯ãƒªã‚¹ãƒã‚¹
- **çº¯æ•°å­—æˆ–æ— æ„ä¹‰å­—ç¬¦** (âš ï¸æ³¨æ„ï¼šå¦‚æœæ•°å­—ä»£è¡¨è§’è‰²åå¦‚"37"æˆ–ä½œå“åå¦‚"1999"ç­‰ï¼Œè¯·**ä¿ç•™**ï¼Œä¸è¦è¿‡æ»¤)

### 2. å½’ä¸€åŒ–ï¼ˆæ”¾å…¥ synonymsï¼‰
å°†åŒä¹‰è¯æ˜ å°„åˆ° **Danbooru é£æ ¼** æ ‡å‡†æ ‡ç­¾ï¼š
- æ ¼å¼: å…¨å°å†™ + ä¸‹åˆ’çº¿ (snake_case)
- **å¤šè¯­è¨€åˆå¹¶**: å¿…é¡»å°†ä¸­æ–‡/æ—¥æ–‡çš„ä½œå“åã€è§’è‰²ååˆå¹¶ä¸ºç»Ÿä¸€çš„è‹±æ–‡/ç½—é©¬éŸ³æ ‡ç­¾
- **ä¼˜å…ˆä½¿ç”¨ä¸Šæ–¹è¯åº“ä¸­çš„æ ‡å‡†å½¢å¼**ï¼Œå¦‚æœè¯åº“ä¸­æ²¡æœ‰ï¼Œå†è‡ªè¡Œåˆ¤æ–­

### 3. ä¿ç•™ï¼ˆä¸å‡ºç°åœ¨è¾“å‡ºä¸­ï¼‰
ä»…ä¿ç•™æ— æ³•å½’ç±»æˆ–æ— éœ€æ ‡å‡†åŒ–çš„æè¿°æ€§æ ‡ç­¾ï¼ˆä¸”ä¸å±äºåŒä¹‰è¯ï¼‰ï¼š
- ç‹¬ç‰¹çš„é£æ ¼æè¿°
- å…·ä½“çš„åœºæ™¯ç»†èŠ‚
- **æ³¨æ„**ï¼šå¦‚æœä¸€ä¸ªæ ‡ç­¾å¯ä»¥è¢«å½’ä¸€åŒ–ï¼ˆå¦‚ä½œå“åï¼‰ï¼Œå¿…é¡»å½’ä¸€åŒ–ï¼Œ**ä¸è¦**ä¿ç•™åŸæ ·ï¼

## è¾“å…¥
```json
{json.dumps(tags, ensure_ascii=False)}
```

## è¾“å‡ºè¦æ±‚
ä¸¥æ ¼ JSONï¼Œæ—  Markdown åŒ…è£¹ï¼š
```json
{{
  "meaningless": ["tag1", "tag2"],
  "synonyms": {{"åŸtag": "æ ‡å‡†tag"}}
}}
```
å¦‚æœæŸç±»åˆ«ä¸ºç©ºï¼Œä½¿ç”¨ç©ºæ•°ç»„/å¯¹è±¡ã€‚"""


# å°† _build_prompt æ–¹æ³•ç»‘å®šåˆ° AITagProcessor ç±»
AITagProcessor._build_prompt = lambda self, tags: _build_ai_prompt(tags)


class XPProfiler:
    """XPç”»åƒæ„å»ºå™¨"""
    
    def __init__(
        self,
        client: PixivClient,
        stop_words: Optional[list[str]] = None,
        discovery_rate: float = 0.1,
        time_decay_days: int = 180,
        ai_config: Optional[dict] = None,
        saturation_threshold: float = 0.5
    ):
        self.client = client
        self.stop_words = set(stop_words or [])
        self.discovery_rate = discovery_rate
        self.time_decay_days = time_decay_days
        self.ai_processor = AITagProcessor(ai_config or {})
        self.saturation_threshold = saturation_threshold  # é«˜é¢‘ Tag é¥±å’Œåº¦é˜ˆå€¼
        self._blocked_artist_ids: set[int] = set()  # åˆå§‹åŒ–ï¼Œç”± load_blacklist å¡«å……
        
        # æ·»åŠ é»˜è®¤åœç”¨è¯ï¼ˆå½’ä¸€åŒ–ä¸ºå°å†™ï¼‰
        # Pixiv å¸¸è§æ— æ„ä¹‰æ ‡ç­¾
        default_stop_words = [
            # é€šç”¨æè¿°
            "original", "ã‚ªãƒªã‚¸ãƒŠãƒ«", "manga", "æ¼«ç”»", "pixiv",
            "illustration", "ã‚¤ãƒ©ã‚¹ãƒˆ", "ç·´ç¿’", "practice",
            "è½æ›¸ã", "doodle", "sketch", "ã‚¹ã‚±ãƒƒãƒ",
            "drawing", "art", "artwork", "fanart", "ãƒ•ã‚¡ãƒ³ã‚¢ãƒ¼ãƒˆ",
            "digital", "ãƒ‡ã‚¸ã‚¿ãƒ«", "ã‚¢ãƒŠãƒ­ã‚°", "analog",
            
            # åˆ†çº§æ ‡ç­¾
            "R-18", "R-18G", "R18", "NSFW", "SFW", "safe",
            
            # æ•°å­—/ç¼–å·ç±»
            "1000userså…¥ã‚Š", "500userså…¥ã‚Š", "100userså…¥ã‚Š", "50userså…¥ã‚Š",
            "5000userså…¥ã‚Š", "10000userså…¥ã‚Š", "userså…¥ã‚Š",
            "1000bookmarks", "500bookmarks", "100bookmarks",
            
            # æ´»åŠ¨/æ¯”èµ›æ ‡ç­¾
            "ã‚³ãƒ³ãƒ†ã‚¹ãƒˆ", "contest", "ä¼ç”»", "project",
            "ãŠé¡Œ", "ãƒªã‚¯ã‚¨ã‚¹ãƒˆ", "request", "commission",
            "è½æ›¸ãé›†", "ã¾ã¨ã‚", "è©°ã‚åˆã‚ã›", "log",
            
            # å¹³å°æ ‡ç­¾
            "twitter", "fanbox", "patreon", "skeb",
            "pixivfanbox", "fantia",
            
            # é€šç”¨å½¢å®¹è¯
            "cute", "kawaii", "ã‹ã‚ã„ã„", "å¯æ„›ã„",
            "beautiful", "ç¶ºéº—", "pretty", "sexy",
            "cool", "ã‹ã£ã“ã„ã„", "ã‚«ãƒƒã‚³ã‚¤ã‚¤",
            
            # å…¶ä»–æ— æ„ä¹‰
            "girls", "girl", "boy", "boys", "woman", "man",
            "female", "male", "solo", "1girl", "1boy",
            "2girls", "2boys", "multiple_girls", "multiple_boys",
            "èƒŒæ™¯", "background", "é¢¨æ™¯", "landscape",
            "å‰µä½œ", "ã‚ªãƒªã‚­ãƒ£ãƒ©", "original_character", "oc",
            "ã†ã¡ã®å­", "çœ‹æ¿å¨˜", "ç‰ˆæ¨©", "äºŒæ¬¡å‰µä½œ",
            "ä»•äº‹çµµ", "ãŠä»•äº‹", "work",
        ]
        for word in default_stop_words:
            self.stop_words.add(word.lower().replace(" ", "_"))
            
    async def load_blacklist(self):
        """ä»æ•°æ®åº“åŠ è½½é»‘åå• (ä»…åŒ…æ‹¬ç”¨æˆ·æ‰‹åŠ¨å±è”½çš„)"""
        try:
            # 1. ä»…åŠ è½½æ‰‹åŠ¨å±è”½çš„æ ‡ç­¾
            # ç”¨æˆ·æ˜ç¡®è¦æ±‚ï¼šæ²¡ç¡®è®¤å°±ä¸å±è”½ï¼Œå› æ­¤ä¸åŠ è½½ high-dislike counts
            blocked_tags = await db.get_blocked_tags()
            for tag in blocked_tags:
                self.stop_words.add(self._normalize_tag(tag))
            
            # 2. åŠ è½½å±è”½çš„ç”»å¸ˆ ID
            blocked_artists = await db.get_blocked_artists()
            self._blocked_artist_ids = {artist_id for artist_id, _ in blocked_artists}
            
            logger.info(f"å·²åŠ è½½é»‘åå•: {len(blocked_tags)} ä¸ªæ‰‹åŠ¨å±è”½Tag + {len(blocked_artists)} ä¸ªå±è”½ç”»å¸ˆ")
        except Exception as e:
            logger.error(f"åŠ è½½é»‘åå•å¤±è´¥: {e}")
            self._blocked_artist_ids = set()

            logger.error(f"åŠ è½½é»‘åå•å¤±è´¥: {e}")
            self._blocked_artist_ids = set()
    
    async def build_profile(
        self,
        user_id: int,
        scan_limit: int = 500,
        include_private: bool = False
    ) -> dict[str, float]:
        """
        æ‰«ææ”¶è—ï¼Œæ„å»ºXPæƒé‡å­—å…¸
        
        Args:
            user_id: ç›®æ ‡ç”¨æˆ·ID
            scan_limit: æ‰«ææ”¶è—æ•°é‡
            include_private: åŒ…å«ç§å¯†æ”¶è—
        
        Returns:
            {tag: weight} æƒé‡å­—å…¸
        """
        # è·å–æ”¶è—
        await self.load_blacklist()  # ç¡®ä¿åŠ è½½æœ€æ–°é»‘åå•
        
        # 1. åŠ è½½æœ¬åœ°ç¼“å­˜ ID
        cached_rows = await db.get_xp_bookmarks(user_id)
        cached_ids = {row['illust_id'] for row in cached_rows}
        
        # 2. æ£€æŸ¥åŒæ­¥çŠ¶æ€
        sync_key = f"sync_completed_{user_id}"
        is_completed = await db.get_state(sync_key) == "true"
        
        # å®šä¹‰ Cursor Key ç”Ÿæˆå™¨
        def get_cursor_key(is_private):
            suffix = "private" if is_private else "public"
            return f"resume_cursor_{user_id}_{suffix}"

        # é€šç”¨è·å–é€»è¾‘å°è£…
        async def fetch_segment(is_private):
            cursor_key = get_cursor_key(is_private)
            saved_cursor = await db.get_state(cursor_key)
            
            # ç­–ç•¥åˆ¤æ–­
            if is_completed:
                mode = "update" # å¢é‡æ›´æ–°
                stop_ids = cached_ids
                skip_ids = None
                start_url = None
                do_tail_resume = False
            elif saved_cursor:
                mode = "jump"   # é«˜æ•ˆè·³è½¬
                stop_ids = cached_ids
                skip_ids = None
                start_url = None
                do_tail_resume = True
            else:
                mode = "slow"   # æ…¢é€Ÿæ‰«æ (é¦–æ¬¡æˆ–ä¸¢å¤±æ¸¸æ ‡)
                stop_ids = None
                skip_ids = cached_ids
                start_url = None
                do_tail_resume = False
                
            desc = "ç§å¯†" if is_private else "å…¬å¼€"
            logger.info(f"[{desc}] æ¨¡å¼: {mode}, ç¼“å­˜: {len(cached_ids)}")

            # å›è°ƒå·¥å‚: åŒºåˆ† Head æ›´æ–°è¿˜æ˜¯ Tail æ›´æ–°
            def make_callback(update_cursor_key=None):
                async def _cb(items, next_url):
                    await db.save_xp_bookmarks(user_id, items)
                    if update_cursor_key and next_url:
                        await db.set_state(update_cursor_key, next_url)
                return _cb

            fetched = []
            
            # 1. å¤´éƒ¨æ‰«æ (å¡«è¡¥æœ€æ–°çš„ Gap)
            # å³ä½¿åœ¨ update æ¨¡å¼ï¼Œä¹Ÿæ˜¯è·‘è¿™ä¸ªã€‚
            # å¦‚æœæ˜¯ jump æ¨¡å¼ï¼Œè¿™é‡Œè´Ÿè´£åªæŠ“æœ€æ–°çš„ï¼Œä¸è¦è¦†ç›– saved_cursor
            logger.info(f"[{desc}] æ­£åœ¨æ‰«æå¤´éƒ¨...")
            head_items = await self.client.get_bookmarks(
                user_id,
                limit=scan_limit,
                private=is_private,
                stop_ids=stop_ids,
                skip_ids=skip_ids,
                start_url=start_url,
                on_batch=make_callback(update_cursor_key=cursor_key if mode == "slow" else None)
            )
            fetched.extend(head_items)
            
            # 2. å°¾éƒ¨è·³è½¬ (ä»… Jump æ¨¡å¼)
            if do_tail_resume:
                logger.info(f"[{desc}] âš¡ è§¦å‘é«˜æ•ˆæ–­ç‚¹ç»­ä¼ ï¼Œç›´æ¥è·³è½¬åˆ°: {saved_cursor[:60]}...")
                # è¿™é‡Œå¿…é¡»ä» saved_cursor å¼€å§‹ï¼Œå¹¶ä¸”è¦æ›´æ–° cursor_key (æ¨è¿›è¿›åº¦)
                # ä¾ç„¶ä¼ å…¥ stop_ids=cached_idsï¼Œä¸‡ä¸€ Ñ…Ğ²Ğ¾å·´ä¹Ÿæ¥ä¸Šäº†å‘¢
                tail_items = await self.client.get_bookmarks(
                    user_id,
                    limit=scan_limit, # å‰©ä½™é¢åº¦ï¼Ÿæš‚ä¸ç²¾ç¡®æ§åˆ¶
                    private=is_private,
                    start_url=saved_cursor,
                    stop_ids=cached_ids,
                    on_batch=make_callback(update_cursor_key=cursor_key)
                )
                fetched.extend(tail_items)
                
            return fetched

        # 4. æ‰§è¡Œè·å–
        bookmarks = await fetch_segment(False)
        
        if include_private and self.client._logged_in:
            private_bookmarks = await fetch_segment(True)
            bookmarks = bookmarks + private_bookmarks
        
        # 5. æ ‡è®°åŒæ­¥å®Œæˆ
        if not is_completed:
            await db.set_state(sync_key, "true")
            logger.info("âœ… å…¨é‡åŒæ­¥å®Œæˆï¼Œæ ‡è®°ä¸º [å·²å®Œæˆ]")
            # æ¸…ç†æ¸¸æ ‡ï¼Ÿå¯é€‰ã€‚ç•™ç€ä¹Ÿæ²¡äº‹ï¼Œä¸‹æ¬¡ is_completed=True ä¼šå¿½ç•¥å®ƒã€‚
        
        # 6. é‡æ–°æ„å»ºå…¨é‡åˆ—è¡¨
        cached_rows = await db.get_xp_bookmarks(user_id)
        
        analyzed_illusts = []
        for row in cached_rows:
            # æ•°æ®åº“é‡Œå­˜çš„æ—¶é—´å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è½¬æ¢
            cdate = row['illust_create_date']
            if isinstance(cdate, str):
                try:
                    cdate = datetime.fromisoformat(cdate)
                except:
                    cdate = datetime.now()
            
            # é¢„å¤„ç†æ ‡ç­¾ï¼šå»é™¤ userså…¥ã‚Š åç¼€ç­‰
            raw_tags = json.loads(row['tags'])
            cleaned_tags = [self._normalize_tag(t) for t in raw_tags]
            # è¿‡æ»¤ç©ºæ ‡ç­¾å¹¶å»é‡ï¼ˆä¿æŒé¡ºåºï¼‰
            cleaned_tags = list(dict.fromkeys(t for t in cleaned_tags if t))
            
            analyzed_illusts.append(Illust(
                id=row['illust_id'],
                title="Cached",
                user_id=user_id,
                user_name="",
                tags=cleaned_tags,
                bookmark_count=0,
                view_count=0,
                page_count=1,
                image_urls=[],
                is_r18=False,
                ai_type=0,
                create_date=cdate
            ))
            
        bookmarks = analyzed_illusts
        logger.info(f"XPåˆ†ææ•°æ®æº: {len(bookmarks)} ä¸ªæ”¶è—ä½œå“ (å«æœ¬åœ°å†å²)")
        
        # ç»Ÿè®¡Tagå‡ºç°æ¬¡æ•°å’Œæ—¶é—´ (å­˜å‚¨ illust_id ç”¨äºæ­£ç¡®è®¡ç®— DF)
        # æ”¯æŒæƒé‡ç³»æ•° (liked items = 0.5xï¼Œå› ä¸º apply_feedback å·²ç»™è¿‡ 1.0x)
        tag_occurrences: dict[str, list[tuple[int, datetime, float]]] = defaultdict(list)
        
        # è·å–å·²ç‚¹èµçš„ä½œå“ID (é¿å…åŒå€è®¡åˆ†)
        liked_ids = await db.get_liked_illusts()
        
        for illust in bookmarks:
            # å·²ç‚¹èµçš„ä½œå“ç»™ 0.5x æƒé‡ (ä¸åé¦ˆçš„ 1.0 åˆè®¡ = 1.5x)
            weight_mult = 0.5 if illust.id in liked_ids else 1.0
            
            for tag in illust.tags:
                normalized = self._normalize_tag(tag)
                if normalized and normalized not in self.stop_words:
                    tag_occurrences[normalized].append((illust.id, illust.create_date, weight_mult))
        
        # AI å¤„ç†ï¼šè¿‡æ»¤æ— æ„ä¹‰tagå’Œå½’ç±»åŒä¹‰tag
        if self.ai_processor.enabled:
            all_tags = list(tag_occurrences.keys())
            valid_tags, synonym_map = await self.ai_processor.process_tags(all_tags)
            
            # åˆå¹¶åŒä¹‰tagçš„ç»Ÿè®¡
            new_occurrences = defaultdict(list)
            for tag, occs in tag_occurrences.items():
                if tag in synonym_map:
                    # æ˜ å°„åˆ°è§„èŒƒåŒ–tag
                    new_occurrences[synonym_map[tag]].extend(occs)
                elif tag in valid_tags:
                    new_occurrences[tag].extend(occs)
                # else: è¢«è¿‡æ»¤çš„æ— æ„ä¹‰tagï¼Œä¸¢å¼ƒ
            
            tag_occurrences = new_occurrences
            logger.info(f"AIå¤„ç†åå‰©ä½™ {len(tag_occurrences)} ä¸ªæœ‰æ•ˆTag")
        
        # è®¡ç®—æƒé‡
        total_docs = len(bookmarks)
        profile = {}
        tag_df = {}  # ç”¨äº PMI è®¡ç®—
        
        # å…ˆè®¡ç®—æ‰€æœ‰ Tag çš„ DF å¹¶æ£€æµ‹é¥±å’Œåº¦
        saturated_tags = []
        for tag, occurrences in tag_occurrences.items():
            unique_illusts = set(item[0] for item in occurrences)
            df = len(unique_illusts)
            tag_df[tag] = df
            
            # é¥±å’Œåº¦æ£€æµ‹ï¼šé«˜é¢‘ Tag è‡ªåŠ¨åŠ å…¥åœç”¨è¯
            saturation = df / total_docs if total_docs > 0 else 0
            if saturation > self.saturation_threshold:
                saturated_tags.append((tag, saturation))
                self.stop_words.add(tag)
        
        if saturated_tags:
            logger.info(f"ğŸ¯ é¥±å’Œåº¦æ£€æµ‹ï¼š{len(saturated_tags)} ä¸ªé«˜é¢‘ Tag è‡ªåŠ¨åŠ å…¥åœç”¨è¯")
            for tag, sat in saturated_tags[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                logger.info(f"   - {tag}: {sat:.1%}")
        
        for tag, occurrences in tag_occurrences.items():
            if tag in self.stop_words:
                continue  # è·³è¿‡é¥±å’Œ Tag
                
            unique_illusts = set(item[0] for item in occurrences)
            dates = [item[1] for item in occurrences]
            weights = [item[2] for item in occurrences]  # æƒé‡ç³»æ•°
            
            weight = self._calculate_weight(
                term_frequency=len(occurrences),
                document_frequency=len(unique_illusts),
                total_documents=total_docs,
                occurrence_dates=dates,
                weight_multipliers=weights  # ä¼ å…¥æƒé‡ç³»æ•°
            )
            profile[tag] = weight
        
        # è®¡ç®—Tagç»„åˆæƒé‡ (Co-occurrence)
        pair_counts = Counter()
        for illust in bookmarks:
            # è·å–è¯¥ä½œå“æ‰€æœ‰æœ‰æ•ˆçš„ normalized tags
            valid_tags = []
            for tag in illust.tags:
                norm = self._normalize_tag(tag)
                if norm and norm not in self.stop_words:
                    valid_tags.append(norm)
            
            # ç»Ÿè®¡ç»„åˆ (åªç»Ÿè®¡é«˜é¢‘Tagçš„ç»„åˆä»¥å‡å°‘å™ªéŸ³)
            if len(valid_tags) >= 2:
                # æ’åºä»¥ä¿è¯ (A, B) å’Œ (B, A) è§†ä¸ºåŒä¸€ä¸ªç»„åˆ
                valid_tags.sort()
                # ç”Ÿæˆæ‰€æœ‰ä¸¤ä¸¤ç»„åˆ
                for t1, t2 in itertools.combinations(valid_tags, 2):
                    # ä»…å½“ä¸¤ä¸ªTagéƒ½åœ¨Profileä¸­æœ‰ä¸€å®šæƒé‡æ—¶æ‰ç»Ÿè®¡ï¼ˆä¾‹å¦‚ Top 50ï¼‰
                    # è¿™é‡Œç®€åŒ–ä¸ºï¼šæ‰€æœ‰ç»„åˆéƒ½ç»Ÿè®¡ï¼Œä½†åç»­æ ¹æ®é¢‘ç‡ç­›é€‰
                    pair_counts[(t1, t2)] += 1
        
        # ä¿å­˜çƒ­é—¨ç»„åˆ (ä½¿ç”¨ PMI ä¼˜åŒ–æƒé‡)
        pairs_to_save = []
        for (t1, t2), count in pair_counts.most_common(100):  # æ‰©å¤§å€™é€‰æ± 
            # è®¡ç®— PMI = log(P(t1,t2) / (P(t1) * P(t2)))
            p_t1 = tag_df.get(t1, 1) / total_docs if total_docs > 0 else 0
            p_t2 = tag_df.get(t2, 1) / total_docs if total_docs > 0 else 0
            p_joint = count / total_docs if total_docs > 0 else 0
            
            # é˜²æ­¢é™¤é›¶ï¼Œä½¿ç”¨å¹³æ»‘
            pmi = math.log(p_joint / (p_t1 * p_t2 + 1e-10) + 1e-10)
            
            # ç»“åˆ PMI å’ŒåŸæƒé‡ï¼ŒPMI ä¸ºè´Ÿè¡¨ç¤ºåç›¸å…³ï¼Œè¿‡æ»¤æ‰
            if pmi > 0:
                weight = pmi * (profile.get(t1, 0) + profile.get(t2, 0))
                pairs_to_save.append((t1, t2, weight))
        
        # åªä¿ç•™ Top 50
        pairs_to_save = sorted(pairs_to_save, key=lambda x: x[2], reverse=True)[:50]
            
        await db.update_xp_tag_pairs(pairs_to_save)
        
        # ============ å†·å¯åŠ¨å¤„ç†ï¼šæ”¶è—å°‘æ—¶æ³¨å…¥çƒ­é—¨ Tag å¼±å…ˆéªŒ ============
        cold_start_threshold = 50  # æ”¶è—å°‘äºæ­¤æ•°æ—¶è§¦å‘å†·å¯åŠ¨
        if len(bookmarks) < cold_start_threshold:
            logger.info(f"ğŸ§Š æ£€æµ‹åˆ°å†·å¯åŠ¨åœºæ™¯ (æ”¶è—: {len(bookmarks)} < {cold_start_threshold})")
            try:
                popular_tags = await db.get_popular_tags(20)
                injected_count = 0
                for tag, freq in popular_tags:
                    normalized_tag = self._normalize_tag(tag)
                    if normalized_tag and normalized_tag not in profile and normalized_tag not in self.stop_words:
                        # å¼±å…ˆéªŒæƒé‡ï¼šé¢‘ç‡ * 0.1ï¼ˆä¸ä¼šå‹è¿‡çœŸå®æ”¶è—ï¼‰
                        prior_weight = freq * 0.1
                        profile[normalized_tag] = prior_weight
                        injected_count += 1
                if injected_count > 0:
                    logger.info(f"   æ³¨å…¥ {injected_count} ä¸ªçƒ­é—¨ Tag ä½œä¸ºå¼±å…ˆéªŒ")
            except Exception as e:
                logger.warning(f"å†·å¯åŠ¨æ³¨å…¥å¤±è´¥: {e}")
        
        # ä¿å­˜åˆ°æ•°æ®åº“ (ç°æœ‰ä»£ç )
        await db.update_xp_profile(profile)
        
        logger.info(f"æ„å»ºXPç”»åƒå®Œæˆï¼Œå…± {len(profile)} ä¸ªTagï¼Œ{len(pairs_to_save)} ä¸ªçƒ­é—¨ç»„åˆ")
        return profile
    
    # é¢„ç¼–è¯‘æ­£åˆ™ï¼šå»é™¤ userså…¥ã‚Š åç¼€
    _pattern_users = re.compile(r"^(.*?)\d+userså…¥ã‚Š$", re.IGNORECASE)
    
    def _normalize_tag(self, tag: str) -> str:
        """
        Tagå½’ä¸€åŒ–
        1. å»é™¤ xxxuserså…¥ã‚Š åç¼€
        2. ç»Ÿä¸€è½¬å°å†™
        3. å»é™¤ç©ºæ ¼
        4. åˆ«åæ˜ å°„
        """
        tag = tag.strip()
        
        # å»é™¤ userså…¥ã‚Š åç¼€
        match = self._pattern_users.match(tag)
        if match:
            prefix = match.group(1)
            if prefix:  # å¦‚æœå‰ç¼€éç©ºï¼Œä½¿ç”¨å‰ç¼€
                tag = prefix
        
        tag = tag.lower()
        
        # æ£€æŸ¥åˆ«åæ˜ å°„
        for alias, canonical in TAG_ALIASES.items():
            if tag == alias.lower():
                return canonical
        
        # æ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿
        tag = tag.replace(" ", "_")
        
        return tag
    
    def _calculate_weight(
        self,
        term_frequency: int,
        document_frequency: int,
        total_documents: int,
        occurrence_dates: list[datetime],
        weight_multipliers: list[float] = None
    ) -> float:
        """
        æƒé‡è®¡ç®—ï¼ˆä¼˜åŒ–åçš„ TF-IDF + æ—¶é—´è¡°å‡ï¼‰
        
        weight = weighted_TF Ã— IDF
        - weighted_TF = Î£(time_decay Ã— weight_mult)
        - IDF = log(N / (df + 1)) + 1  (å¸¦å¹³æ»‘çš„æ ‡å‡†IDF)
        """
        now = datetime.now(occurrence_dates[0].tzinfo if occurrence_dates else None)
        
        # 0. é«˜é¢‘é¥±å’Œåº¦è¿‡æ»¤ (å¦‚æœè¶…è¿‡é˜ˆå€¼å¦‚ 50%ï¼Œè§†ä¸ºæ— æ„ä¹‰åœç”¨è¯)
        df_ratio = document_frequency / total_documents if total_documents > 0 else 0
        if df_ratio > self.saturation_threshold:
            return 0.0

        # è®¡ç®—å¸¦æ—¶é—´è¡°å‡çš„ TF (å«æƒé‡ç³»æ•°)
        weighted_tf = 0
        for i, date in enumerate(occurrence_dates):
            days_ago = (now - date).days
            # ç¡®ä¿ days_ago ä¸ä¸ºè´Ÿ
            days_ago = max(0, days_ago)
            decay = math.exp(-days_ago / self.time_decay_days)
            # åº”ç”¨æƒé‡ç³»æ•° (liked items = 0.5x)
            weighted_tf += decay * weight_multipliers[i]
        
        # 1. å¯¹ TF åº”ç”¨å¯¹æ•°æŠ‘åˆ¶ (é˜²æ­¢æ•°é‡å †ç§¯å¯¼è‡´çš„çº¿æ€§æ— é™å¢é•¿)
        # log10(1 + 10) = 1.04
        # log10(1 + 100) = 2.0
        # log10(1 + 500) = 2.7
        # å³ä½¿æœ‰ 1000 ä¸ªæ”¶è—ï¼Œæƒé‡ä¹Ÿåªæ¯” 100 ä¸ªå¤š 35%ï¼Œè€Œä¸æ˜¯ 1000%
        if weighted_tf > 0:
            weighted_tf = math.log10(1 + weighted_tf)
        
        # 2. æ ‡å‡† IDF (å¸¦å¹³æ»‘é˜²æ­¢é™¤é›¶)
        idf = math.log(total_documents / (document_frequency + 1)) + 1
        
        return weighted_tf * idf
    
    async def get_top_tags(self, n: int = 20) -> list[tuple[str, float]]:
        """è·å–æƒé‡æœ€é«˜çš„Nä¸ªTag"""
        profile = await db.get_xp_profile()
        sorted_tags = sorted(profile.items(), key=lambda x: x[1], reverse=True)
        return sorted_tags[:n]
    
    async def apply_feedback(self, illust: Illust, action: str, config: dict):
        """
        åº”ç”¨ç”¨æˆ·åé¦ˆè°ƒæ•´æƒé‡
        
        Args:
            illust: ä½œå“å¯¹è±¡
            action: 'like' | 'dislike'
            config: åé¦ˆé…ç½®
        """
        like_boost = config.get("like_boost", 0.5)
        dislike_penalty = config.get("dislike_penalty", 0.3)
        dislike_threshold = config.get("dislike_threshold", 3)
        
        # è·å–å½“å‰ç”»åƒç”¨äºåˆ†çº§æƒ©ç½š
        profile = await db.get_xp_profile()
        max_weight = max(profile.values()) if profile else 1
        
        suggested_block_tag = None  # è®°å½•å»ºè®®å±è”½çš„ Tag
        
        for tag in illust.tags:
            normalized = self._normalize_tag(tag)
            if not normalized or normalized in self.stop_words:
                continue
            
            if action == "like":
                await db.adjust_tag_weight(normalized, like_boost)
                logger.debug(f"Tag '{normalized}' æƒé‡ +{like_boost}")
            
            elif action == "dislike":
                # åˆ†çº§æƒ©ç½šï¼šé«˜æƒé‡ Tag å‡å°‘æƒ©ç½šåŠ›åº¦ï¼ˆå¯èƒ½æ˜¯ç”¨æˆ·æ ¸å¿ƒåå¥½ï¼‰
                current_weight = profile.get(normalized, 0)
                weight_ratio = current_weight / max_weight if max_weight > 0 else 0
                # é«˜æƒé‡ Tag æœ€å¤šå‡åŠæƒ©ç½š
                adjusted_penalty = dislike_penalty * (1 - weight_ratio * 0.5)
                
                await db.adjust_tag_weight(normalized, -adjusted_penalty)
                
                # åŒæ—¶æ›´æ–°è´Ÿå‘ç”»åƒï¼ˆç”¨äºä¸»åŠ¨æ’æ–¥ç›¸ä¼¼ä½œå“ï¼‰
                await db.adjust_negative_weight(normalized, adjusted_penalty)
                
                count = await db.increment_tag_dislike(normalized)
                
                # ç”¨æˆ·è¦æ±‚ï¼šä»…ç¡®è®¤ä¸€æ¬¡ï¼Œæ²¡ç¡®è®¤å°±ç®—äº†
                if count == dislike_threshold:
                    logger.info(f"Tag '{normalized}' ç´¯è®¡å¦è®¤ {count} æ¬¡ï¼Œå»ºè®®åŠ å…¥é»‘åå•")
                    # åªè®°å½•ç¬¬ä¸€ä¸ªè¾¾åˆ°é˜ˆå€¼çš„ Tag
                    if suggested_block_tag is None:
                        suggested_block_tag = normalized
                elif count > dislike_threshold:
                    logger.debug(f"Tag '{normalized}' ç´¯è®¡å¦è®¤ {count} æ¬¡ (å·²æç¤ºè¿‡)")
                else:
                    logger.debug(f"Tag '{normalized}' æƒé‡ -{adjusted_penalty:.2f} (åˆ†çº§æƒ©ç½š)")
        
        # ç”»å¸ˆæƒé‡å…³è” (Artist Weight) - åªæ‰§è¡Œä¸€æ¬¡
        if illust.user_id:
            try:
                artist_delta = 1.0 if action == "like" else -1.0
                await db.update_artist_score(illust.user_id, artist_delta)
                logger.debug(f"ç”»å¸ˆ {illust.user_id} ({illust.user_name}) æƒé‡ {artist_delta:+.1f}")
            except Exception as e:
                logger.error(f"æ›´æ–°ç”»å¸ˆæƒé‡å¤±è´¥: {e}")

        # è®°å½•åé¦ˆ - åªæ‰§è¡Œä¸€æ¬¡
        await db.record_feedback(illust.id, action)
        
        return suggested_block_tag
